---
title: "coachella_aspen_1"
author: "ricardo_piedrahita"
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4),Sys.Date(),'.html'))})
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
    self_contained: yes
    code_folding: hide
---

# First script in sequence of 3.
## Make sure to copy the QAQC output into the Filter Analysis_MR.xlsx file to ensure the most recent filter data is analyzed with the correct sample information.
## Load data


```{r setup, include=TRUE,echo=TRUE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.width = 8, fig.height = 4)

graphics.off()
email = 0 # Set to 1 to send out (only rp has permissions, others will fail)
## Analyze Coachella ASPEN data
# Import data, perform QAQC check.  If email == 1, send an email out.
#Set directory with this file as the working directory.
# Saving all output to the shared field folder.  Code remains separate/isolated.
dir.create("~/Dropbox/Coachella Field/Data/processing/tester",showWarnings = F)
dir.create("~/Dropbox/Coachella Field/Data/processing/odk data",showWarnings = F)  #Put odk data downloads into here.
dir.create("~/Dropbox/Coachella Field/Data/processing/plots",showWarnings = F)
dir.create("~/Dropbox/Coachella Field/Data/processing/processed data",showWarnings = F)
dir.create("~/Dropbox/Coachella Field/Data/processing/QA Reports",showWarnings = F)
dir.create("~/Dropbox/Coachella Field/Data/processing/results",showWarnings = F)


#Keys: instrument_id, location, size

source('r_scripts/load.R')
source('r_scripts/ASPEN_functions.R')
email = 0 #Set to 1 to send out summary qaqc email, else 0


```

# Import data

```{r import, include=TRUE,echo=TRUE}


#Import Excel meta data (instrument start/stop times, filter IDs, etc.)
field_log = readxl::read_xlsx(list.files(path = "~/Dropbox/Coachella Field/Data",pattern = 'AspenData', recursive = T, full.names = T),
                              skip = 2,col_names  = TRUE,sheet = 'Sample Form Data') %>%
  clean_names() %>% 
  dplyr::mutate(across(contains("date") | contains("time"), as.character)) %>%
  dplyr::mutate(across(contains("time"), str_remove, "1899-12-31")) %>%
  dplyr::mutate(
    datetime_setup = as.POSIXct(paste(setup_date, setup_time),
                                tz="America/Los_Angeles"),
    datetime_start = case_when(!is.na(programmed_sample_start_date) ~
                                 as.POSIXct(paste(programmed_sample_start_date,
                                                  programmed_sample_start_time),
                                            format="%Y-%m-%d",
                                            tz="America/Los_Angeles")),
    datetime_end   = case_when(!is.na(programmed_sample_end_date) ~
                                 as.POSIXct(paste(programmed_sample_end_time,
                                                  programmed_sample_end_time),
                                            format="%Y-%m-%d",
                                            tz="America/Los_Angeles"))) %>% 
  dplyr::select(-programmed_sample_start_time,-programmed_sample_start_date,-programmed_sample_end_time,-programmed_sample_end_date) %>% 
  tidyr::pivot_longer(cols = c("pm2_5_quartz_filter_id","pm10_quartz_filter_id",
                               "pm2_5_teflon_filter_id","pm10_teflon_filter_id",
                               "teflon_blank_id","quartz_blank_id"),
                      names_to = "type",
                      values_to="filter_id") %>% 
  dplyr::mutate( filter_type = ifelse(type %like% "quartz","quartz","teflon")) %>% 
  dplyr::rename_all(function(x) paste0("excel_", x)) %>% 
  dplyr::mutate(cartridge_id = toupper(excel_filter_id),
                excel_comments = toupper(excel_comments)) %>% 
  dplyr::filter(!is.na(cartridge_id)) %>% 
  dplyr::distinct()

saveRDS(field_log,"../Data/field_log.rds")

# Summary of samples collected.  Maybe add an explicit flag for removing data with less than 48h runtime?
DT::datatable(field_log %>% 
                dplyr::group_by(excel_type) %>% 
                dplyr::summarise(count = n()),
              caption = "Samples collected excluding blanks")


#Import list of filters assembled in Colorado
filter_list = readxl::read_xlsx(list.files(path = "~/Dropbox/Coachella Field/Data",
                                           pattern = 'AspenData', recursive = T, full.names = T),
                                skip = 0,sheet = 'Filter List')[,2:7] %>% 
  clean_names() %>% 
  dplyr::rename(cartridge_id = cartridge,
                filter_type = type) %>% 
  dplyr::filter(!is.na(shipment_to_coachella)) %>% 
  unite(filter_id,c("filter_id","filter_id_cu")) %>% 
  dplyr::mutate(shipment_to_coachella = as.Date(shipment_to_coachella),
                shipment_to_colorado = as.Date(shipment_to_colorado),
                filter_id = gsub("_NA","",filter_id),
                filter_id = gsub("NA_","",filter_id),
                filter_id = toupper(filter_id),
                cartridge_id = toupper(cartridge_id))
# filter_id = case_when(filter_type == "Quartz" ~ paste0(as.character(shipment_to_coachella),"_",cartridge_id),
#                      TRUE ~ filter_id))
DT::datatable(filter_list,caption="Complete list of filters used in study")


#Import ASPEN data

aspen_files = c(list.files(path = "~/Dropbox/Coachella Field/Data/Field Data Collected",
                           pattern = 'ASPEN00', recursive = T, full.names = T) %>% 
                    grep("Firmware|Berkley", ., invert = TRUE ,value = TRUE, ignore.case = TRUE),
                list.files(path = "~/Dropbox/Coachella Field/Data/Community Samples",
                           pattern = 'ASPEN00', recursive = T, full.names = T))

#Huge, don't need this atm.
# aspen_data = rbindlist(lapply(aspen_files,read_aspen),fill=TRUE) 

aspen_files[sapply(aspen_files, file.size) > 10000]

aspen_header <- rbindlist(lapply(aspen_files,read_aspen_header),fill=TRUE) %>% 
  as.data.frame() %>% 
  dplyr::rename(Filter1ID = CIDfilter1,
                Filter2ID = CIDfilter2) %>% 
  dplyr::select(ASPENserial,LogFilename,Filter1ID,Filter2ID,Filter1StartDateTimeUTC,Filter2StartDateTimeUTC,
                Filter1LastUpdateUTC,Filter2LastUpdateUTC,
                Filter1ShutdownMode,Filter2ShutdownMode,Filter1SampledRuntime,Filter2SampledRuntime,Filter1AverageVolumetricFlowRate,
                Filter2AverageVolumetricFlowRate,Filter1SampledVolume,Filter2SampledVolume) 

aspen_header <- rbind(aspen_header %>% 
                        dplyr::select(ASPENserial,LogFilename,starts_with("Filter1")) %>% 
                        dplyr::mutate(Filter = "Filter1") %>% 
                        dplyr::rename_all(function(x) gsub("Filter1","", x)),
                      aspen_header %>% 
                        dplyr::select(ASPENserial,LogFilename,starts_with("Filter2")) %>% 
                        dplyr::mutate(Filter = "Filter2") %>% 
                        dplyr::rename_all(function(x) gsub("Filter2","", x))) %>% 
  dplyr::rename_all(function(x) paste0("file_", x)) %>% 
  dplyr::rename(cartridge_id = file_ID) %>% 
  dplyr::arrange(file_LogFilename,file_StartDateTimeUTC) %>% 
  dplyr::mutate(file_start_date = as.Date(file_StartDateTimeUTC),
                file_end_date = as.Date(file_LastUpdateUTC),
                cartridge_id = toupper(cartridge_id),
                SizeCut = case_when(file_Filter %like% "Filter1" ~ "PM2.5",
                                    file_Filter %like% "Filter2" ~ "PM10",
                                    TRUE ~ "NA")) %>% 
  dplyr::mutate(StartDateTimeLocal = with_tz(file_StartDateTimeUTC, 
                                             tzone="America/Los_Angeles"),
                date_start = as.Date(StartDateTimeLocal),
                EndDateTimeLocal = with_tz(file_LastUpdateUTC, 
                                             tzone="America/Los_Angeles"),
                date_end = as.Date(EndDateTimeLocal)) %>%
  dplyr::filter(as.Date(StartDateTimeLocal) %in% as.Date(field_log$excel_datetime_start) | 
                  hour(StartDateTimeLocal) == 0 | 
                  hour(StartDateTimeLocal) == 1) %>% 
  distinct()

saveRDS(aspen_header,"../Data/aspen_header.rds")


```

# Data summaries of individual data streams

```{r summarization, include=TRUE,echo=TRUE}

files_summary <- aspen_header %>% 
  dplyr::group_by(SizeCut,file_ASPENserial) %>% 
  dplyr::summarise(n_samples_good = sum(file_SampledRuntime>20,na.rm = T),
                   n_samples_short = sum(file_SampledRuntime<20,na.rm = T)) %>% 
  dplyr::rename(sample_type = SizeCut,
                filter_type = file_ASPENserial) %>% 
  dplyr::mutate(filter_type = case_when(filter_type %like% "0022"~"Teflon",
                                        TRUE ~ "Quartz"))

field_log_summary <- field_log %>% 
  dplyr::group_by(excel_type) %>% 
  dplyr::summarise(n_samples_attempted = sum(!is.na(cartridge_id),na.rm = T)) %>% 
  dplyr::rename(samplefilter_type = excel_type) %>% 
  dplyr::mutate(filter_type = case_when(samplefilter_type %like% "teflon"~"Teflon",
                                        TRUE ~ "Quartz"),
                sample_type = case_when(samplefilter_type %like% "pm10"~"PM10",
                                        samplefilter_type %like% "pm2_5"~"PM2.5",
                                        TRUE ~ "blank")) %>% 
  dplyr::select(filter_type,sample_type,n_samples_attempted) %>% 
  dplyr::left_join(files_summary) 

kable(field_log_summary,"pipe",caption = "Overvie of samples collected")



```

# Merge and format data


```{r merge, include=TRUE,echo=TRUE}

#1. Check the filter IDs.  Are they consistent between the log sheet and the instrument?
#2. Check the sample operation - sample duration, flow rate, any error messages.  Done.
#3.  Send this info to Madeleine and Christian and Edgar. Done
#4. Send reminder on how when to ship filters.


#Merge the filter ID list with the field log sheet.
field_log_filter_list <- dplyr::left_join(field_log,filter_list, by="cartridge_id") %>% 
  #This should be negative for all.  And then keep the nearest or NA.  Date sent it TO Cali.
  dplyr::mutate(excel_sent_datediff = as.Date(shipment_to_coachella)-as.Date(excel_datetime_start), 
                excel_run_between_shipments = if_else(as.Date(excel_datetime_start) > as.Date(shipment_to_coachella) &
                                                        as.Date(excel_datetime_start) < as.Date(shipment_to_colorado),
                                                      TRUE,
                                                      FALSE),
                sampletype = case_when(excel_type %like% "BLANK|blank|Blank" ~ "blank",
                                       TRUE ~ "ambient")) %>% 
  dplyr::filter(excel_run_between_shipments == TRUE)  %>% 
  dplyr::arrange(cartridge_id,desc(sampletype),excel_sent_datediff) %>% 
  dplyr::mutate(R_notes_filterlist = 
                  case_when(excel_sent_datediff>0 ~ "Ensure that the Excel file is up to date, and not merging with another row on Cartridge ID incorrectly.",
                            TRUE ~ "NA"))  %>%
  dplyr::select(filter_id, everything())


#1 Merge file with meta data.  And then with filter List to make sure it exists therein




aspen_header_merged_temp = dplyr::full_join(aspen_header,field_log_filter_list,by = c("cartridge_id")) %>% 
  dplyr::mutate(volume_flag = case_when(!file_AverageVolumetricFlowRate %between% c(1.8,2.2) ~ 1,
                                        !file_AverageVolumetricFlowRate %between% c(1.8,2.2) ~ 1,
                                        is.na(file_AverageVolumetricFlowRate) | 
                                          is.na(file_AverageVolumetricFlowRate) ~ 0 ),
                meta_filter_merge_flag = ifelse(is.na(excel_location),1,0),
                duration_flag = ifelse(!file_SampledRuntime %between% c(43.2,52.8),1,0)) %>% 
  dplyr::select(cartridge_id,file_ASPENserial,file_LogFilename,
                file_StartDateTimeUTC,file_start_date,file_ShutdownMode,file_SampledRuntime,
                file_AverageVolumetricFlowRate,file_SampledVolume,excel_datetime_start,
                excel_filter_type,excel_type,filter_type,filter_id,SizeCut,volume_flag,
                meta_filter_merge_flag,duration_flag,R_notes_filterlist,excel_comments,
                shipment_to_coachella,shipment_to_colorado,excel_run_between_shipments) %>%
  dplyr::rowwise() %>% 
  #Trust the Excel spreadsheet over the aspen file headers...
  dplyr::mutate(type_match_log_headers = excel_type %like% "BLANK|blank|Blank" | 
                  grepl(substr(excel_type,1,3),tolower(substr(SizeCut,1,3)))) %>% 
  dplyr::filter(type_match_log_headers) %>% 
  #This should be negative for all.  And then keep the nearest or NA.  Date sent it TO Cali.
  dplyr::mutate(excel_sent_datediff = as.Date(shipment_to_coachella)-as.Date(excel_datetime_start), 
                excel_run_between_shipments = if_else(as.Date(excel_datetime_start) > as.Date(shipment_to_coachella) &
                                                        as.Date(excel_datetime_start) < as.Date(shipment_to_colorado),
                                                      TRUE,
                                                      FALSE),
                sampletype = case_when(excel_type %like% "BLANK|blank|Blank" ~ "blank",
                                       TRUE ~ "ambient"),
                sizecut_str = tolower(substr(SizeCut,1,3))) %>% 
  dplyr::rowwise() %>% 
  dplyr::mutate(type_match = grepl(sizecut_str, excel_type)) %>%
  dplyr::mutate(filtertype_match = grepl(tolower(filter_type), excel_type)) %>%
  dplyr::filter(filtertype_match |
                  type_match | 
                  excel_run_between_shipments == TRUE |
                  (is.na(file_start_date) | #Keep NA file start dates since some files were not saved.
                     (file_SampledRuntime>2 | sampletype %like% "blank")))  %>% #Runs less than 1 hour are from tests.
  #This is difference between excel's start date, and the file's.  Should be 0.
  dplyr::mutate(excel_file_datediff = as.Date(excel_datetime_start) - as.Date(file_start_date)) %>% #Should be negative, and nearest!
  dplyr::arrange(file_StartDateTimeUTC,cartridge_id,desc(sampletype),file_start_date,excel_file_datediff) %>% 
  dplyr::mutate(R_notes = case_when(excel_sent_datediff>0 ~ "Ensure that the Excel file is up to date, and not merging with another row on Cartridge ID incorrectly.",
                                    excel_file_datediff>0 ~ "ASPEN file may be missing, impute data as possible"))  %>%
  dplyr::select(filter_id, everything())


#Remove filters with bad merges with the file metadata (like if a file is missing)
aspen_header_merged <- aspen_header_merged_temp %>% 
  #Keep only the filters that were used after they were mailed to Cali.
  dplyr::filter(!sampletype %like% "blank",
                excel_sent_datediff < 0,
                (is.na(excel_file_datediff) | abs(excel_file_datediff) < 2))  %>% #Set to 2 to give some leeway in bad data entry.
  dplyr::group_by(cartridge_id,file_start_date) %>% 
  dplyr::arrange(desc(excel_sent_datediff)) %>% 
  # Keep the nearest matched cartridge used on a given day
  dplyr::filter(1 ==  row_number()) %>% 
  #Keep the nearest filter used of the merged cartridge/used-on dates.
  dplyr::group_by(filter_id) %>% 
  dplyr::arrange(desc(excel_sent_datediff)) %>% 
  dplyr::mutate(row_number = row_number()) %>% 
  dplyr::filter(row_number == 1)

aspen_header_merged_blanks <- aspen_header_merged_temp %>% 
  #Keep only the filters that were used after they were mailed to Cali.
  dplyr::filter(sampletype %like% "blank",
                excel_sent_datediff < 0)  %>% 
  #Keep the nearest filter used of the merged cartridge/used-on dates.
  dplyr::group_by(filter_id) %>% 
  dplyr::arrange(desc(excel_sent_datediff)) %>% 
  dplyr::filter(row_number() == 1)

DT::datatable(rbind(aspen_header_merged,aspen_header_merged_blanks),caption = "Merged data set - REVIEW")


filename = paste0("~/Dropbox/Coachella Field/Data/QA Reports/Indio_QA_Report_",  Sys.Date(), ".xlsx")
openxlsx::write.xlsx(list(complete_list = aspen_header_merged_temp,clean_list = rbind(aspen_header_merged,aspen_header_merged_blanks)),
           file = filename)


if(email == 1){
  source('r_scripts/emailfun.R')
  emailfun(email,filename)
}


```
